---
title: "Classification Techniques"
author: "Katie Daisey"
output: html_notebook
---

```{r, setup}
datafile <- "/Users/katiedaisey/Desktop/data/chemometrics/Archaeometry_data_2007.csv"
#datafile <- "/Users/katiedaisey/Desktop/local/data/chemometrics/Archaeometry_data_2007.csv"
hmlcdir <- "/Users/katiedaisey/Desktop/hmlc"

library(class)
library(caret)
library(ggplot2)
library(pls)
library(randomForest)
library(rpart)
library(devtools)
#install(hmlcdir)
#library(hmlc)
```


# Data

```{r}
data <- read.csv(datafile)
train <- data[c(T,F),]
test <- data[c(F,T),]
```

```{r}
str(data)
```

There are 188 samples each with 11 measured variables. Two "variables" are actual information about the sample, sample name and sample class. The remaining 9 variables are measurements of various elements, likely concentrations of some sort (ie parts per million or parts per billion).

```{r}
matplot(t(scale(data[,3:11], center = T)), type = "l", lty = 1, col = data[,2], 
        xaxt = "n", ylab = "Relative Values")
axis(1, at = 1:9, labels = colnames(data[,3:11]), las = 2)
boxplot(data[data[,2] == 3,3:11], col = 3)
table(data[,2])
```


```{r}
palette <- rainbow(n = 7)
ggplot(data, aes(Class)) + geom_bar(fill = palette) 
```

But the samples are not evenly distributed across the classes. Class 4 only has 2 samples while Class 2 has 56 samples. As might be expected, this makes classification of Class 4 samples difficult.


# k-Nearest Neighbors

The simplest classification technique is k-nearest neighbors, or knn.

```{r knn_example}
colors <- palette[train[,2]]

plot(train[,5:6], pch = 20, col = palette[train[,2]])
points(test[1,5:6], pch = 4, col = 1, cex = 2)
```
```{r}
plot(scale(train[,5:6], scale = T), pch = 20, col = palette[train[,2]])
```

```{r}
knn_test <- knn(train = train[,3:11], test = test[,3:11], cl = train[,2], k = 1)
plot(train[,5:6], pch = 20, col = palette[train[,2]])
points(test[1,5:6], pch = 4, col = palette[knn_test][1], cex = 2)
```
```{r}
plot(train[,5:6], pch = 20, col = palette[train[,2]])
points(test[,5:6], pch = 4, col =1, cex = 2)
plot(train[,5:6], pch = 20, col = palette[train[,2]])
points(test[,5:6], pch = 4, col = palette[knn_test], cex = 2)
```


```{r}
plot(1:94, knn_test, pch = 20, col = palette[test[,2]],
     ylab = "Predicted Class", xlab = "Sample No.", yaxt = 'n')
axis(2, at = 1:7, labels = rep("", times = 7))
text(-7, y = 1:7, labels = 1:7,col=palette[1:7], xpd = T, srt = 0)
```

```{r}
knn_test <- knn(train = train[,3:11], test = test[,3:11], cl = train[,2], k = 1)
plot(1:94, knn_test, pch = 20, col = palette[test[,2]],
                xlab = "Sample No", ylab = "Predicted Class", yaxt = 'n')
axis(2, at = 1:7, labels = rep("", times = 7))
text(-7, y = 1:7, labels = 1:7,col=palette[1:7], xpd = T, srt = 0)

```




```{r}
scaled_train <- scale(train[,3:11])
scaled_test <- scale(test[,3:11], center = attributes(scaled_train)$`scaled:center`, scale = attributes(scaled_train)$`scaled:scale`)

knn_test <- knn(train = scaled_train, test = scaled_test, cl = train[,2], k = 1)
plot(1:94, knn_test, pch = 20, col = palette[test[,2]],
     xlab = "Sample No.", ylab = "Predicted Class", yaxt = 'n')
axis(2, at = 1:7, labels = rep("", times = 7))
text(-7, y = 1:7, labels = 1:7,col=palette[1:7], xpd = T, srt = 0)
```




# PCA Projection

```{r}
pca1 <- prcomp(train[,3:11], center = F, scale. = F)
plot(pca1)

#pairs(pca1$x, pch = 20, col = palette[train[,2]])
```


```{r}
new_data <- as.matrix(test[,3:11]) %*% pca1$rotation

plot(pca1$x[,1:2], pch = 20, col = palette[train[,2]])
points(new_data[,1:2], pch = 4, col = palette[test[,2]])

knn1 <- knn(train = pca1$x, test = new_data, cl = train[,2])
plot(pca1$x[,1:2], pch = 20, col = palette[train[,2]])
points(new_data[,1:2], pch = 4, col = palette[knn1])
plot(pca1$x[,2:3], pch = 20, col = palette[train[,2]])
points(new_data[,2:3], pch = 4, col = palette[knn1])

knn2 <- knn(train = pca1$x[,1:3], test = new_data[,1:3], cl = train[,2])
plot(pca1$x[,1:2], pch = 20, col = palette[train[,2]])
points(new_data[,1:2], pch = 4, col = palette[knn2])
plot(pca1$x[,2:3], pch = 20, col = palette[train[,2]])
points(new_data[,2:3], pch = 4, col = palette[knn2])

knn3 <- knn(train = pca1$x[,2:7], test = new_data[,2:7], cl = train[,2])
plot(pca1$x[,2:3], pch = 20, col = palette[train[,2]])
points(new_data[,2:3], pch = 4, col = palette[knn3])

plot(1:94, knn1, pch = 20, col = palette[test[,2]])
plot(1:94, knn2, pch = 20, col = palette[test[,2]])
plot(1:94, knn3, pch = 20, col = palette[test[,2]])
```


```{r}
pca1 <- prcomp(scaled_train, center = F, scale. = F)
plot(pca1)
new_data <- as.matrix(scaled_test) %*% pca1$rotation

plot(pca1$x[,1:2], pch = 20, col = palette[train[,2]])
points(new_data[,1:2], pch = 4, col = palette[test[,2]])

knn1 <- knn(train = pca1$x, test = new_data, cl = train[,2])
plot(pca1$x[,1:2], pch = 20, col = palette[train[,2]])
points(new_data[,1:2], pch = 4, col = palette[knn1])

knn2 <- knn(train = pca1$x[,1:3], test = new_data[,1:3], cl = train[,2])
plot(pca1$x[,1:2], pch = 20, col = palette[train[,2]])
points(new_data[,1:2], pch = 4, col = palette[knn2])

knn3 <- knn(train = pca1$x[,2:6], test = new_data[,2:6], cl = train[,2])
plot(pca1$x[,1:2], pch = 20, col = palette[train[,2]])
points(new_data[,1:2], pch = 4, col = palette[knn3])

plot(1:94, knn1, pch = 20, col = palette[test[,2]])
plot(1:94, knn2, pch = 20, col = palette[test[,2]])
plot(1:94, knn3, pch = 20, col = palette[test[,2]])
```



# Partial Least Squares (Discriminant Analysis)

 

```{r}
train2 <- data.frame(Class = train[,2])
train2$Data <- as.matrix(train[,3:11])

test2 <- data.frame(Class = test[,2])
test2$Data <- as.matrix(test[,3:11])
```


```{r}
pls1 <- plsr(Class~Data, data = train2, ncomp = 9, validation = "CV")
plot(pls1, plottype = "validation")
pls1 <- predict(pls1, newdata = test2$Data, comps = 4)

plot(1:94, pls1, pch = 20, col = palette[test2$Class], 
     ylim = c(1,7), xlab = "Sample No.", ylab = "Predicted Class")
```


The more correct way to do classification with PLS is PLSDA - Partial Least Squares Discriminant Analysis.

```{r}
color <- iris$Species
color <- cbind(color, 0)
color <- cbind(color, 0)
color <- cbind(color, 0)
color[color[,1] == 1,2] <- 1
color[color[,1] == 2,3] <- 1
color[color[,1] == 3,4] <- 1
color[,2:4] <- color[,2:4] + 1
plot(iris[,3:4], pch = 20, col = color[,1])
a <- lm(iris[,4]~iris[,3])
abline(a[[1]][1],a[[1]][[2]], col = "red")
plot(iris[,3:4], pch = 20, col = color[,2])
plot(iris[,3:4], pch = 20, col = color[,3])
plot(iris[,3:4], pch = 20, col = color[,4])

pca1 <- prcomp(iris[,3:4])
plot(iris[,3:4])
abline(0,1/0.387)
pca1$rotation
```


```{r}
?plsda
pls2 <- plsda(train2$Data, as.factor(train2$Class), ncomp = 9, type = "class")
pls2 <- predict(pls2, newdata = test2$Data, ncomp = 4)
plot(1:94, pls2, pch = 20, col = palette[test2$Class],
     xlab = "Sample No.", ylab = "Predicted Class")

```



# CART

```{r}
tmp1 <- as.data.frame(train2$Data)
tmp2 <- as.data.frame(test2$Data)
cart1 <- rpart(Class~Data, data = train2, method = "class")
cart1 <- predict(cart1, newdata = test2, type = "class")
plot(1:94, cart1, pch = 20, col = palette[test2$Class])
```




# Random Forest

```{r}
rF1 <- randomForest(x = train2$Data, y = train2$Class)
rF1 <- predict(rF1, newdata = test2$Data)

plot(1:94, rF1, pch = 20, col = palette[test$Class])
```


```{r}
rF1 <- randomForest(x = train2$Data, y = as.factor(train2$Class))
rF1 <- predict(rF1, newdata = test2$Data)
plot(1:94, rF1, pch = 20, col = palette[test2$Class])
```





# HMLC

```{r}

gen_hmlc_tree <- function(classes, data, dist_method = "euclidean", hclust_method = "ward.D2") {
  
  tmp_df <- data.frame(Class = classes)
  tmp_df$Data <- data
  centers <- aggregate(Data~Class, data = tmp_df, FUN = mean)
  
  h1 <- stats:::hclust(stats:::dist(centers, method = dist_method), method = hclust_method)
  h1 <- hmlc:::hclust_to_hltree(h1)
  
  return(h1)
}
# make tree
h1 <- gen_hmlc_tree(train2$Class, train2$Data)

flatten <- function(results, leaf_nodes) {
  results <- results[,leaf_nodes]
  
  out <- rep(0, length(results[,1]))
  
  for (i in leaf_nodes) {
    out[results[,i] == 1] <- i
  }
  
  return(out)
}  

labels <- create_labels(train2$Class, hltree = h1, depth = "full")
```




```{r}
hmlc1 <- hmlc:::hsc.lcn(data = train2$Data, test = test2$Data, labels = labels, hltree = h1, policy = "actual", classifier = "knn")
a <- flatten(hmlc1, 1:7)
plot(1:94, a, pch = 20, col = palette[test2$Class])
```


```{r}
hmlc1 <- hmlc:::hsc.lcn(data = train2$Data, test = test2$Data, labels = labels, hltree = h1, policy = "actual", classifier = "rpart")
a <- flatten(hmlc1, 1:7)
plot(1:94, a, pch = 20, col = palette[test2$Class])
```



```{r}
hmlc1 <- hmlc:::hsc.lcn(data = train2$Data, test = test2$Data, labels = labels, hltree = h1, policy = "actual", classifier = "randomForest")
a <- flatten(hmlc1, 1:7)
plot(1:94, a, pch = 20, col = palette[test2$Class])
```




```{r}
hmlc1 <- hmlc:::hlcl(data = train2$Data, test = test2$Data, labels = labels, hltree = h1, policy = "actual", classifier = "randomForest")
a <- flatten(hmlc1, 1:7)
plot(1:94, a, pch = 20, col = palette[test2$Class])
```




```{r}
hmlc1 <- hmlc:::hlcpn(data = train2$Data, test = test2$Data, labels = labels, hltree = h1, policy = "siblings", classifier = "randomForest")
a <- flatten(hmlc1, 1:7)
plot(1:94, a, pch = 20, col = palette[test2$Class])
```



```{r}
hmlc1 <- hmlc:::hsc.lcn(data = train2$Data, test = test2$Data, labels = labels, hltree = h1, policy = "actual", classifier = "svm")
a <- flatten(hmlc1, 1:7)
plot(1:94, a, pch = 20, col = palette[test2$Class])
```








